{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de99273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please note that the parameters below have to be adjusted for specific datasets\n",
    "\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7aa3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"MNIST\" # either \"CIFAR\" or \"MNIST\" or \"UCIHAR\" or \"ISOLET\"\n",
    "max_length = 1000 # Dataset will be shortened to max_length if too large\n",
    "\n",
    "attempt_using_GPU = True # If GPU is available, it will be used\n",
    "\n",
    "simple_NN = False # If True, only a very small NN with one hidden layer will be used\n",
    "                    # Otherwise, a CNN will be used. For CIFAR, a CNN is needed\n",
    "\n",
    "load_tensor_Hamiltonians = True # All available gates will be loaded as (potentially large) tensors\n",
    "                                # Depending on your hardware, this becomes problematic at n=8+ qubits\n",
    "\n",
    "calculate_density_matrices = False # calculate the full 2**n by 2**n density matrices for getting \n",
    "                                  # the trace distance (True) or just use 2**n size states (False)\n",
    "\n",
    "save_data = False # saves loss and gradient to .csv file\n",
    "\n",
    "load_symbolic_hamiltonians = True # Load available hamiltonians as list of symbolic strings (instead of matrices)\n",
    "\n",
    "use_symbolic_operations = True # If true, this code will use symbolic operations\n",
    "                                # see functions ending with _symbolic\n",
    "if attempt_using_GPU:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732a31af",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_check(device, use_symbolic_operations, load_symbolic_hamiltonians, load_tensor_Hamiltonians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3111bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definitions of basic quantum operators, states, and constants\n",
    "I, X, Y, Z = get_pauli_matrices(device)\n",
    "zero, one = get_quantum_states(device)\n",
    "h_bar = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8859447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2 # number of qubits\n",
    "m = n + n + n*(n-1)//2 # number of hamiltonians that will be needed for the system\n",
    "                       # one X control for every qubit, one Z control for every qubit\n",
    "                       # 2 pairwise Z controls for every 2 qubits (n choose 2)\n",
    "\n",
    "shape = (m, 2**n, 2**n) # Shape of the tensor containing all the hamiltonians\n",
    "T = 1 # Time during which hamiltonians are applied\n",
    "steps = 5  # Number of discrete values of the activation functions for each hamiltonian\n",
    "trotter_number = 2 # This is the variable TN is my notes\n",
    "\n",
    "K = 10 # Number of classes â€“ must be between 2 and the total number of classes given in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1f2b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_symbolic_hamiltonians:\n",
    "    Hamiltonians_symbolic = generate_list_of_hamiltonians_symbolic_form(n)\n",
    "    \n",
    "if load_tensor_Hamiltonians:\n",
    "    Hamiltonians = generate_list_of_hamiltonians_matrix_form(n, shape, I, X, Z, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a7f76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See class 'embedder' for details\n",
    "embedding = embedder(n, Hamiltonians, m, T, steps, trotter_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7716b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_accepted = [str(x) for x in range(K)]\n",
    "print(\"The following classes are added to the dataset:\\n\", classes_accepted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398017a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test, nn_input_dimensions = load_and_preprocess_data(dataset, device, classes_accepted, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5751fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Y_train\")\n",
    "for i in range(max(Y_train)+1):\n",
    "    if Y_train.count(i) > 0:\n",
    "        print(i,\"occurs\",Y_train.count(i),\"times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd224aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Y_test\")\n",
    "for i in range(max(Y_test)+1):\n",
    "    if Y_test.count(i) > 0:\n",
    "        print(i,\"occurs\",Y_test.count(i),\"times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693b3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if K != len(set(Y_test)): # Number of distinct classes\n",
    "    print(\"There seems to be an error here!\")\n",
    "print(\"There are\", K, \"classes in total in the\", dataset, \"dataset.\")\n",
    "\n",
    "if K != max(Y_test)+1:\n",
    "    print(\"Warning! classes in the dataset don't seem to be labeled from 0 to K\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd61abc3",
   "metadata": {},
   "source": [
    "# Defining the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46a5950",
   "metadata": {},
   "outputs": [],
   "source": [
    "if simple_NN:\n",
    "    classical_net = Classical_Net_simple(nn_input_dimensions, dataset, K).to(device)\n",
    "else:\n",
    "    classical_net = Classical_Net_conv(nn_input_dimensions, dataset, K).to(device)\n",
    "\n",
    "print(\"Classical NN:\\n\\n\",classical_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f01f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(classical_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a93159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrects = 0\n",
    "predictions = classical_classify(X_train, classical_net)\n",
    "\n",
    "for i in range(len(Y_train)):\n",
    "    if predictions[i] == Y_train[i]:\n",
    "        corrects += 1\n",
    "\n",
    "print(\"Training accuracy before classical NN training = \", 100*corrects/len(Y_train),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4632fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrects = 0\n",
    "predictions = classical_classify(X_test, classical_net)\n",
    "\n",
    "for i in range(len(Y_test)):\n",
    "    if predictions[i] == Y_test[i]:\n",
    "        corrects += 1\n",
    "\n",
    "print(\"Test accuracy before classical NN training = \", 100*corrects/len(Y_test),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06961a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(classical_net.parameters(), lr=0.01, weight_decay=1e-1) #MNIST\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss_history = []\n",
    "batch_size = len(X_train)\n",
    "all_labels = torch.zeros((len(Y_train),K))\n",
    "for i in range(len(Y_train)):\n",
    "    all_labels[i,Y_train[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bfb7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 10000\n",
    "for epoch in range(max_epochs): # loop over the dataset multiple times\n",
    "    \n",
    "    random_indices = random.sample(range(len(X_train)), batch_size)\n",
    "    \n",
    "    output = classical_net(X_train[random_indices])\n",
    "    labels = all_labels[random_indices].type(torch.FloatTensor).to(device)\n",
    "    \n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # forward + backward + optimize\n",
    "    loss = criterion(output,labels)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_history.append(loss.detach().to(device='cpu'))\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411854b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(loss_history, label = \"Loss function\")\n",
    "plt.ylabel(\"Training loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc350f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrects = 0\n",
    "predictions = classical_classify(X_train, classical_net)\n",
    "\n",
    "for i in range(len(Y_train)):\n",
    "    if predictions[i] == Y_train[i]:\n",
    "        corrects += 1\n",
    "\n",
    "print(\"Training accuracy after classical NN training = \", 100*corrects/len(Y_train),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd85db2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corrects = 0\n",
    "predictions = classical_classify(X_test, classical_net)\n",
    "\n",
    "for i in range(len(Y_test)):\n",
    "    if predictions[i] == Y_test[i]:\n",
    "        corrects += 1\n",
    "\n",
    "print(\"Test accuracy after classical NN training = \", 100*corrects/len(Y_train),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a17847",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 120\n",
    "if simple_NN:\n",
    "    net = Hybrid_Net_simple(nn_input_dimensions, m, steps, hidden_layer_size, dataset).to(device)\n",
    "else:\n",
    "    net = Hybrid_Net_conv(nn_input_dimensions, m, steps, hidden_layer_size, dataset).to(device)\n",
    "    \n",
    "print(\"Quantum NN:\\n\\n\",net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5268ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_output = net(X_train)\n",
    "nn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825662a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = torch.reshape(nn_output.type(torch.complex64),(len(nn_output),m,steps))\n",
    "activation_functions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f4bcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, classes_density_matrices_or_states, _ = nn_loss(X_train, Y_train, net, m, n, steps, K, T, save_data, calculate_density_matrices, use_symbolic_operations, embedding, Hamiltonians, Hamiltonians_symbolic, h_bar, device, trotter_number)\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc4937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = sum([classify_multiclass(X_train[i], net, classes_density_matrices_or_states, calculate_density_matrices, T, m, n, steps, trotter_number, device, Hamiltonians_symbolic, use_symbolic_operations, embedding, K) == Y_train[i] for i in range(len(Y_train))])\n",
    "accuracy = (correct_predictions / len(Y_train)) * 100\n",
    "print(\"Training accuracy before hybrid NN training = \", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b29237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = sum([classify_multiclass(X_test[i], net, classes_density_matrices_or_states, calculate_density_matrices, T, m, n, steps, trotter_number, device, Hamiltonians_symbolic, use_symbolic_operations, embedding, K) == Y_test[i] for i in range(len(Y_test))])\n",
    "accuracy = (correct_predictions / len(Y_test)) * 100\n",
    "print(\"Test accuracy before hybrid NN training = \", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05757b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if simple_NN:\n",
    "    net.fc1.weight = copy.deepcopy(classical_net.fc1.weight)\n",
    "    net.fc1.bias = copy.deepcopy(classical_net.fc1.bias)\n",
    "else:\n",
    "    net.conv1.weight = copy.deepcopy(classical_net.conv1.weight)\n",
    "    net.conv1.bias = copy.deepcopy(classical_net.conv1.bias)\n",
    "\n",
    "    net.pool = copy.deepcopy(classical_net.pool)\n",
    "\n",
    "    net.conv2.weight = copy.deepcopy(classical_net.conv2.weight)\n",
    "    net.conv2.bias = copy.deepcopy(classical_net.conv2.bias)\n",
    "\n",
    "    net.fc1.weight = copy.deepcopy(classical_net.fc1.weight)\n",
    "    net.fc1.bias = copy.deepcopy(classical_net.fc1.bias)\n",
    "\n",
    "    net.fc2.weight = copy.deepcopy(classical_net.fc2.weight)\n",
    "    net.fc2.bias = copy.deepcopy(classical_net.fc2.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0917be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, classes_density_matrices_or_states, _ = nn_loss(X_train, Y_train, net, m, n, steps, K, T, save_data, calculate_density_matrices, use_symbolic_operations, embedding, Hamiltonians, Hamiltonians_symbolic, h_bar, device, trotter_number)\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab6a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = sum([classify_multiclass(X_train[i], net, classes_density_matrices_or_states, calculate_density_matrices, T, m, n, steps, trotter_number, device, Hamiltonians_symbolic, use_symbolic_operations, embedding, K) == Y_train[i] for i in range(len(Y_train))])\n",
    "accuracy = (correct_predictions / len(Y_train)) * 100\n",
    "print(\"Training accuracy of hybrid NN using classical pre-training = \", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec909988",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrects = 0\n",
    "counter = 0\n",
    "for i in range(len(Y_train)):\n",
    "    counter += 1\n",
    "    if classify_multiclass(X_train[i], net, classes_density_matrices_or_states, calculate_density_matrices, T, m, n, steps, trotter_number, device, Hamiltonians_symbolic, use_symbolic_operations, embedding, K) == Y_train[i]:\n",
    "        corrects += 1\n",
    "print(\"Training accuracy of hybrid NN using classical pre-training = \", 100*corrects/counter,\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d7cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrects = 0\n",
    "counter = 0\n",
    "for i in range(len(Y_test)):\n",
    "    counter += 1\n",
    "    if classify_multiclass(X_test[i], net, classes_density_matrices_or_states, calculate_density_matrices, T, m, n, steps, trotter_number, device, Hamiltonians_symbolic, use_symbolic_operations, embedding, K) == Y_test[i]:\n",
    "        corrects += 1\n",
    "print(\"Test accuracy of hybrid NN using classical pre-training= \", 100*corrects/counter,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b3a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "grad_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d05cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.00001\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=1e-3)\n",
    "batch_size = len(X_train)\n",
    "for epoch in range(10): # loop over the dataset multiple times\n",
    "    \n",
    "    # torch.autograd.set_detect_anomaly(True) # Only needed for debugging, not sure if it slows down the code\n",
    "    \n",
    "    random_indices = random.sample(range(len(X_train)), batch_size)\n",
    "    inputs = X_train[random_indices]\n",
    "    labels = [Y_train[t] for t in random_indices]\n",
    "    \n",
    "    # zero the parameter gradients\n",
    "    start_time = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    end_time = time.time()\n",
    "    print(\"Time to calculate optimizer.zero_grad() function =\", end_time-start_time)\n",
    "    \n",
    "    # forward + backward + optimize\n",
    "    start_time = time.time()\n",
    "    loss, _ , activation_functions = nn_loss(inputs, labels, net, m, n, steps, K, T, save_data, calculate_density_matrices, use_symbolic_operations, embedding, Hamiltonians, Hamiltonians_symbolic, h_bar, device, trotter_number)\n",
    "    end_time = time.time()\n",
    "    print(\"Time to run nn_loss() function =\", end_time-start_time)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    loss.backward()\n",
    "    end_time = time.time()\n",
    "    print(\"Time to calculate loss.backward function =\", end_time-start_time)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    optimizer.step()\n",
    "    end_time = time.time()\n",
    "    print(\"Time to calculate optimizer.step()  =\", end_time-start_time)\n",
    "\n",
    "    # Storing the gradient\n",
    "    if save_data:\n",
    "        grad_temp_sum = torch.mean(torch.abs(activation_functions.grad)) \n",
    "    \n",
    "    # print statistics\n",
    "    if save_data:\n",
    "        print('[%2d] loss: %.9f grad: %.9f' %(epoch, loss, grad_temp_sum))\n",
    "        grad_history.append(grad_temp_sum)\n",
    "    else:\n",
    "        print('[%2d] loss: %.9f' %(epoch, loss))\n",
    "    \n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a0e7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history, label = \"Hilbert-Schmidt\")\n",
    "plt.ylabel(\"Training loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d5a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_data:\n",
    "    file_name = \"n=\"+str(n)+\" lr=\"+str(lr)+\" loss and grad.csv\"\n",
    "    textfile = open(file_name, \"a\")\n",
    "    for i in range(len(loss_history)):\n",
    "        textfile.write(str(i) + \",\" + '%.8f,%.8f\\n'%(loss_history[i], grad_history[i]))\n",
    "    textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d50d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, classes_density_matrices_or_states, _ = nn_loss(X_train, Y_train, net, m, n, steps, K, T, save_data, calculate_density_matrices, use_symbolic_operations, embedding, Hamiltonians, Hamiltonians_symbolic, h_bar, device, trotter_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e18084",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrects = 0\n",
    "counter = 0\n",
    "for i in range(len(Y_train)):\n",
    "    counter += 1\n",
    "    if classify_multiclass(X_train[i], net, classes_density_matrices_or_states, calculate_density_matrices, T, m, n, steps, trotter_number, device, Hamiltonians_symbolic, use_symbolic_operations, embedding, K) == Y_train[i]:\n",
    "        corrects += 1\n",
    "print(\"Training accuracy after NN training = \", 100*corrects/counter,\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df3ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrects = 0\n",
    "counter = 0\n",
    "for i in range(len(Y_test)):\n",
    "    counter += 1\n",
    "    if classify_multiclass(X_test[i], net, classes_density_matrices_or_states, calculate_density_matrices, T, m, n, steps, trotter_number, device, Hamiltonians_symbolic, use_symbolic_operations, embedding, K) == Y_test[i]:\n",
    "        corrects += 1\n",
    "print(\"Test accuracy after NN training = \", 100*corrects/counter,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e941fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f3d013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f9f8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3179ea6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f309d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e39009b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee163e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f6234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c51d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8dd248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
